{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filters\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten, Conv2D, BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import MobileNetV2  \n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from glob import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image size and paths \n",
    "IMAGE_SIZE = [224, 224]\n",
    "train_path = \"C:\\Users\\narendra\\Downloads\\archive_extracted\\Train\"\n",
    "valid_path = \"C:\\Users\\narendra\\Downloads\\archive_extracted\\Test\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobileNetV2 base model \n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=IMAGE_SIZE + [3])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all but the last 10 layers \n",
    "for layer in base_model.layers[:-10]:\n",
    "    layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional layers \n",
    "x = base_model.output\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)  # Extract more spatial features\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)  # Prevent overfitting\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)  # Another convolutional layer\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Flatten()(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layers with ReLU or LeakyReLU activation for classification \n",
    "x = Dense(128, activation='relu')(x)  # Consider LeakyReLU for vanishing gradient issues\n",
    "x = Dropout(0.4)(x)  # Increased dropout rate\n",
    "# Get the number of output classes based on training data folders\n",
    "folders = glob('E://Train/*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output layer with softmax activation for multi-class classification\n",
    "prediction = Dense(len(folders), activation='softmax')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model with the additional layers\n",
    "model = Model(inputs=base_model.input, outputs=prediction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with appropriate metrics and optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation \n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   rotation_range=20,  # Add rotation\n",
    "                                   width_shift_range=0.2,  # Add width/height shift\n",
    "                                   height_shift_range=0.2)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 89046 images belonging to 14 classes.\n",
      "Found 22262 images belonging to 14 classes.\n"
     ]
    }
   ],
   "source": [
    "# Flow data from directories with target size and batch size\n",
    "training_set = train_datagen.flow_from_directory(train_path, target_size=IMAGE_SIZE, batch_size=32, class_mode='categorical')\n",
    "test_set = test_datagen.flow_from_directory(valid_path, target_size=IMAGE_SIZE, batch_size=32, class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Badari\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2783/2783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2913s\u001b[0m 1s/step - accuracy: 0.7066 - loss: 1.0543 - val_accuracy: 0.8601 - val_loss: 1.0772\n",
      "Epoch 2/10\n",
      "\u001b[1m2783/2783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Badari\\anaconda3\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "\u001b[1m2783/2783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2689s\u001b[0m 965ms/step - accuracy: 0.9240 - loss: 0.2691 - val_accuracy: 0.9261 - val_loss: 0.3060\n",
      "Epoch 4/10\n",
      "\u001b[1m2783/2783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 5/10\n",
      "\u001b[1m2783/2783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2593s\u001b[0m 931ms/step - accuracy: 0.9604 - loss: 0.1394 - val_accuracy: 0.9675 - val_loss: 0.1377\n",
      "Epoch 6/10\n",
      "\u001b[1m2783/2783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 7/10\n",
      "\u001b[1m2783/2783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2527s\u001b[0m 907ms/step - accuracy: 0.9742 - loss: 0.0943 - val_accuracy: 0.9760 - val_loss: 0.0884\n",
      "Epoch 8/10\n",
      "\u001b[1m2783/2783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 9/10\n",
      "\u001b[1m2783/2783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2527s\u001b[0m 907ms/step - accuracy: 0.9801 - loss: 0.0737 - val_accuracy: 0.9780 - val_loss: 0.0800\n",
      "Epoch 10/10\n",
      "\u001b[1m2783/2783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# Train the model with callbacks\n",
    "r = model.fit(training_set,\n",
    "              validation_data=test_set,\n",
    "              epochs=10,\n",
    "              steps_per_epoch=len(training_set),\n",
    "              validation_steps=len(test_set),\n",
    "              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "\n",
    "\n",
    "model.save(\"crime_detection_model_optimized_1.h5\")\n",
    "print(\"Model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22262 images belonging to 14 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Badari\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m696/696\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m406s\u001b[0m 580ms/step - accuracy: 0.9795 - loss: 0.0813\n",
      "Test loss: 0.08000743389129639\n",
      "Test accuracy: 0.9780343174934387\n",
      "\u001b[1m696/696\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m405s\u001b[0m 580ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Abuse       1.00      0.60      0.75        58\n",
      "       Arrest       0.97      0.99      0.98       694\n",
      "        Arson       0.97      0.94      0.96       542\n",
      "      Assault       1.00      0.98      0.99       555\n",
      "     Burglary       0.99      0.99      0.99      1535\n",
      "    Explosion       0.93      0.98      0.95      1280\n",
      "     Fighting       1.00      0.93      0.96       240\n",
      " NormalVideos       0.98      1.00      0.99     13110\n",
      "RoadAccidents       1.00      0.96      0.98       541\n",
      "      Robbery       0.95      1.00      0.97       154\n",
      "     Shooting       1.00      0.93      0.97      1477\n",
      "  Shoplifting       0.99      0.87      0.92      1496\n",
      "     Stealing       1.00      0.96      0.98       389\n",
      "    Vandalism       0.99      0.95      0.97       191\n",
      "\n",
      "     accuracy                           0.98     22262\n",
      "    macro avg       0.98      0.93      0.95     22262\n",
      " weighted avg       0.98      0.98      0.98     22262\n",
      "\n",
      "Confusion Matrix:\n",
      "[[   35    16     0     0     0     0     0     7     0     0     0     0\n",
      "      0     0]\n",
      " [    0   684     0     0     0     2     0     8     0     0     0     0\n",
      "      0     0]\n",
      " [    0     0   511     0     1    27     0     3     0     0     0     0\n",
      "      0     0]\n",
      " [    0     0     0   546     0     2     0     1     0     0     0     6\n",
      "      0     0]\n",
      " [    0     0     0     0  1527     4     0     4     0     0     0     0\n",
      "      0     0]\n",
      " [    0     0     1     0    10  1249     1    17     0     0     0     2\n",
      "      0     0]\n",
      " [    0     0     3     0     7     6   223     0     0     1     0     0\n",
      "      0     0]\n",
      " [    0     5     2     0     2     4     0 13092     0     0     2     3\n",
      "      0     0]\n",
      " [    0     0     0     0     0    17     0     7   517     0     0     0\n",
      "      0     0]\n",
      " [    0     0     0     0     0     0     0     0     0   154     0     0\n",
      "      0     0]\n",
      " [    0     0     8     0     2    20     0    63     0     0  1380     4\n",
      "      0     0]\n",
      " [    0     0     0     0     0     8     0   188     0     0     0  1300\n",
      "      0     0]\n",
      " [    0     0     3     0     0     2     0     0     0     7     1     1\n",
      "    373     2]\n",
      " [    0     1     0     0     0     0     0     8     0     0     0     0\n",
      "      0   182]]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Load the model\n",
    "model = load_model(\"crime_detection_model_optimized_1.h5\")\n",
    "\n",
    "# Load the test data \n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    valid_path,\n",
    "    target_size=(IMAGE_SIZE),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False  # Do not shuffle for evaluation\n",
    ")\n",
    "\n",
    "# Evaluate model performance on the test data\n",
    "test_loss, test_accuracy = model.evaluate(test_set)\n",
    "print(\"Test loss:\", test_loss)\n",
    "print(\"Test accuracy:\", test_accuracy)\n",
    "\n",
    "# Predict class labels for the test set\n",
    "predictions = model.predict(test_set)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Get the true labels\n",
    "true_labels = test_set.classes\n",
    "\n",
    "# Get class labels\n",
    "class_labels = list(test_set.class_indices.keys())\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_labels, predicted_classes, target_names=class_labels))\n",
    "\n",
    "# Compute and print confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_classes)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
